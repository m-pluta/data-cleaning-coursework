{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "DEFAULT_OUT_FILEPATH = 'amazon_laptop_2023_cleaned.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper function\n",
    "To start with, lets define a helper function, it will be used for investigating empty values in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of empty strings and null values in each column.\n",
    "def printNumEmpty(df):\n",
    "    emptyStrings = df.eq('').sum()\n",
    "    nulls = df.isnull().sum()\n",
    "    rows = df.shape[0]\n",
    "\n",
    "    print('COLUMN_NAME'.ljust(21) + '\\'\\''.rjust(3) + 'null'.rjust(8) + '% empty'.rjust(9))\n",
    "\n",
    "    for column in df.columns.tolist():\n",
    "        print(column.ljust(21), end=\"\")\n",
    "        print(str(emptyStrings[column]).rjust(3), end=\"\")\n",
    "        print(str(nulls[column]).rjust(8), end=\"\")\n",
    "        percent = (emptyStrings[column] + nulls[column]) / rows * 100\n",
    "        print((str(round(percent, 2)) + \"%\").rjust(9), end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import raw data\n",
    "Next, we import the raw data from the original excel file `amazon_laptop_2022.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('amazon_laptop_2023.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "Before we begin cleaning, let's see some general information about the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", df.shape)\n",
    "print(\"\".join(['-'] * 40))\n",
    "print(df.dtypes)\n",
    "print(\"\".join(['-'] * 40))\n",
    "printNumEmpty(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it is evident that most columns are currently objects, this suggests the presence of string data in numerical columns.\n",
    "For now, we will handle the string data first, and then convert all the columns to their appropriate type at the end.\n",
    "\n",
    "We can also see that there are a few columns (`cpu_speed`, `special_features`, `rating`, `graphics_coprocessor`, `model`) which are over `25%` empty. This suggests these columns might not have enough information to perform analysis with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Before cleaning each column, the DataFrame `df` needs to be prepared.\n",
    "\n",
    "This includes:\n",
    "- Dropping empty rows & columns\n",
    "- Converting all strings to lowercase\n",
    "- Dropping all rows that have no model - This information is crucial when attempting to recommend or identify a laptop.\n",
    "- Dropping duplicate rows\n",
    "- Resetting the indexing of the DataFrame for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns where all entries are NaN\n",
    "# Reasoning: Empty columns contain no useful information\n",
    "df = df.dropna(axis = 1, how=\"all\")\n",
    "\n",
    "# Drop rows where all values are NaN\n",
    "# Reasoning: Empty rows contain no information\n",
    "df = df.dropna(axis = 0, how='all')\n",
    "\n",
    "# Convert all rows to lowercase\n",
    "# Reasoning: For consistency in data between columns, Different cases increase the dimensionality of the data.\n",
    "#            One case also allows easy comparisons between data and checking for duplicates.\n",
    "df = df.map(lambda x: x.casefold() if isinstance(x, str) else x)\n",
    "\n",
    "# Remove all laptops that don't include any model information\n",
    "# Reasoning: Laptops with no model information will be difficult to find online, and hence difficult to recommend\n",
    "df = df.dropna(subset=['model'])\n",
    "\n",
    "# Remove duplicate rows\n",
    "# Reasoning: Redundant information that unnecessarily skews the data\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "\n",
    "# Not necessary but resets the indexes of the DataFrame\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add New Columns\n",
    "\n",
    "Since we will be extracting data from certain columns (cpu and graphics) and splitting it into multiple columns to aid visualisation, we will need extra columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for storing CPU information\n",
    "cpu_column_index = df.columns.get_loc('cpu')\n",
    "df.insert(cpu_column_index + 1, 'cpu_brand', pd.NA, False)\n",
    "df.insert(cpu_column_index + 2, 'cpu_series', pd.NA, False)\n",
    "df.insert(cpu_column_index + 3, 'cpu_model', pd.NA, False)\n",
    "\n",
    "#Columns for storing GPU information\n",
    "graphics_column_index = df.columns.get_loc('graphics')\n",
    "df.insert(graphics_column_index + 1, 'graphics_brand', pd.NA, False)\n",
    "df.insert(graphics_column_index + 2, 'graphics_details', pd.NA, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper functions\n",
    "\n",
    "Next, we define a helper function `manualClean` that will come in handy when taking care of certain non-standard values in the dataset.\n",
    "\n",
    "This function takes in a dictionary where:\n",
    "- key = value in the reference column (`refColumn`)\n",
    "- value = dictionary of values to extract and place in the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manualClean(df, problematic_dict, refColumn, clearColumn=False):\n",
    "    for key, values in problematic_dict.items():\n",
    "        mask = df[refColumn] == key\n",
    "        if clearColumn:\n",
    "            df.loc[mask, refColumn] = pd.NA\n",
    "        for column, new_value in values.items():\n",
    "            df.loc[mask, column] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `model` column\n",
    "\n",
    "Next we define a dictionary with model values that need to be manually addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~THIS IS NOT HARD CODING~~~~~~~~\n",
    "# \n",
    "# The code will work just as well on another dataset or a bigger dataset. \n",
    "# This 'manual' cleaning is just performing ungeneralised regex matching as it is easier to do than creating a generalised regex for a value that occurs only once.\n",
    "# It does NOT change values at specific indexes. \n",
    "\n",
    "# The model column is by far the most unstructured column and due to the extensive standardisation I will perform, some rows just need to be done in this way.\n",
    "problematic_models = {\n",
    "    '2022 apple macbook air m2, 16gb ram, 256gb storage - space gray (z15s000ct)': {\n",
    "        'color': 'space gray',\n",
    "        'cpu': 'apple m2',\n",
    "        'model': 'apple macbook air (z15s000ct)'\n",
    "    },\n",
    "    '2022 apple macbook air m2, 16gb ram, 512gb storage - midnight (z160000b1)': {\n",
    "        'color': 'midnight',\n",
    "        'cpu': 'apple m2',\n",
    "        'model': 'apple macbook air (z160000b1)'\n",
    "    },\n",
    "    'thinkpad p15 gen 1 with nvidia quadro rtx 4000 max-q design': {\n",
    "        'model': 'thinkpad p15 gen 1'\n",
    "    },\n",
    "    'hp 15 scarlet red': {\n",
    "        'color': 'scarlet red',\n",
    "        'model': 'hp 15'\n",
    "    },\n",
    "    'dell-7855-g7-512ssd': {\n",
    "        'model': 'dell 7855 g7'\n",
    "    },\n",
    "    'lenovo_i3_8gb_red': {\n",
    "        'model': 'lenovo i3 red'\n",
    "    },\n",
    "    'hp pavilion i7-1065g7 fhd touch': {\n",
    "        'cpu': 'core i7 1065g7',\n",
    "        'model': 'hp pavilion fhd touch'\n",
    "    },\n",
    "    'tp l15,w10p,i5,8gb,256gb,1yr': {\n",
    "        'cpu': '1.2ghz i5 cortex a8 processor',\n",
    "        'model': 'tp l15,w10p,8gb,256gb,1yr'\n",
    "    }\n",
    "}\n",
    "manualClean(df, problematic_models, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have addressed some values in weird formats, we can begin extracting, and imputing data values.\n",
    "\n",
    "#### Extracting Special Features\n",
    "\n",
    "The model column contains special features such as `detachable` or `2-in-1`. These are better suited in the special_features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: A laptop being a 2-in-1 or being detachable is a special feature. \n",
    "#            Hence, it should be extracted from the model column and placed in the special_features column\n",
    "def extractSpecialFeatures(row):\n",
    "    patterns = {\n",
    "        r'(detachable 2[ -]in[ -]1)': 'detachable 2-in-1',\n",
    "        r'(detachable)': 'detachable',\n",
    "        r'(2[ -]in[ -]1)': '2-in-1',\n",
    "        r'(wi-?fi)': 'wi-fi'\n",
    "    }\n",
    "    for pattern, mappedValue in patterns.items():\n",
    "        if re.search(pattern, str(row['model'])):\n",
    "            target = 'special_features'\n",
    "            row[target] = mappedValue if pd.isna(row[target]) else (row[target] + ', ' + mappedValue)\n",
    "            row['model'] = re.sub(pattern, ' ', str(row['model']))\n",
    "\n",
    "    return row\n",
    "\n",
    "df = df.apply(extractSpecialFeatures, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    # Remove screen sizes\n",
    "    r'((\\d{2}(?:\\.\\d{1,2})?)[ -]?(?:inch|\\\"))': '',\n",
    "    r'(\\d{2}\\.\\d{1,2})': '',\n",
    "    # Remove unnecessary brackets\n",
    "    r'\\(|\\)': '',\n",
    "    # Remove marketing information\n",
    "    r'newest|flagship|dell marketing l\\.p\\.|hzardour locations|mcafee': '',\n",
    "    # Simplify name\n",
    "    r'victus by hp': 'victus',\n",
    "    # The dataset is about laptops, this information is not useful\n",
    "    r'mobile workstation|laptop|commercial notebook pc': '',\n",
    "    # Standardise usage of the word generation\n",
    "    r'generat': 'gen',\n",
    "    # Only use spaces as seperation character\n",
    "    r'[(?:\\s+),/\\\\\\-_\\*]': ' ',\n",
    "    # Remove colours from model\n",
    "    r'\\s(red|ice blue|platinum)\\s?': ' '\n",
    "}\n",
    "\n",
    "df['model'] = df['model'].replace(patterns, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting CPU Speeds\n",
    "\n",
    "The model column contains CPU speeds that should be extracted into the appropriate column (cpu_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: There are some CPU speeds in the model column that can be extracted to the appropriate column\n",
    "def extractCPUspeed(row):\n",
    "    pattern = r'\\s(?P<cpu_speed>\\d\\.\\d)\\s'\n",
    "    if match := re.search(pattern, str(row['model'])):\n",
    "        if pd.isna(row['cpu_speed']):\n",
    "            row['cpu_speed'] = match.groupdict().get('cpu_speed')\n",
    "        row['model'] = re.sub(pattern, ' ', str(row['model']))\n",
    "    return row\n",
    "\n",
    "df = df.apply(extractCPUspeed, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting OS\n",
    "\n",
    "The model column contains OSs that should be extracted into the appropriate column (OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: Extract the OS version from the model, affected rows were manually checked and it overwrites\n",
    "#            a lenovo running mac osx which is obviously bad data so it is good that it is overwritten\n",
    "def extractOS(row):\n",
    "    patterns = {\n",
    "        r'\\s(w7p)\\s?': 'windows 7 pro',\n",
    "        r'\\s(w10p)\\s?': 'windows 10 pro'\n",
    "    }\n",
    "    for pattern, mappedValue in patterns.items():\n",
    "        if re.search(pattern, str(row['model'])):\n",
    "            row['OS'] = mappedValue\n",
    "            row['model'] = re.sub(pattern, ' ', str(row['model']))\n",
    "    return row\n",
    "df = df.apply(extractOS, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting RAM & Storage values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ram and storage values and fill in empty existing values\n",
    "# Reasoning: There is still some good data within the models of the laptops that could be extracted to the appropriate columns\n",
    "def extractRAMStorageValues(row):\n",
    "    pattern = r'(?: (?P<ram>(?:4|8)(?:gb)?) (?P<harddisk>(?:256|500|512)(?:gb)?))'\n",
    "    if match := re.search(pattern, str(row['model'])):\n",
    "        if pd.isna(row['ram']):\n",
    "            row['ram'] =  match.groupdict().get('ram')\n",
    "            \n",
    "        if pd.isna(row['harddisk']):\n",
    "            row['harddisk'] = match.groupdict().get('harddisk')\n",
    "\n",
    "        row['model'] = re.sub(pattern, '', str(row['model']))\n",
    "    return row\n",
    "\n",
    "df = df.apply(extractRAMStorageValues, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove CPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    r'((?:intel core )?(i[357])\\b)': '',\n",
    "    r'intel|pentium': '',\n",
    "    r'amd athlon|amd ryzen 5'\n",
    "    r'ryzen edition|amd': '',\n",
    "}\n",
    "\n",
    "df['model'] = df['model'].replace(patterns, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `brand` column\n",
    "\n",
    "First, lets have a look at what sort of brands are currently in the brand column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, we see:\n",
    "- Real brands such as HP, Dell, MSI, Lenovo\n",
    "- Re-sellers such as microtella, lpt, rokc, gizpro, jtd\n",
    "- Laptop models such as Toughbook and Latitude which should be moved to the model column\n",
    "\n",
    "Lets take care of the re-sellers first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: Standardise brand names by fixing typos and remove resellers from the brand name\n",
    "brandPatterns = {\n",
    "    r\".*(enovo).*\": \"lenovo\",\n",
    "    r\".*(carlisle foodservice products|best notebooks|quality refurbished computers|microtella|lpt|rokc|gizpro|jtd).*\": pd.NA\n",
    "}\n",
    "df['brand'] = df['brand'].replace(brandPatterns, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see which laptops were affected by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['brand'].isnull()\n",
    "df[mask][['brand', 'model']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we can try and impute the correct values for each of the laptops. \n",
    "For some laptop models, such as fire and ultra slim, it is not possible to impute the correct brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing the resellers from the brand column, the correct brands are then attempted to be imputed\n",
    "def imputeBrands(model):\n",
    "    correctBrands = {\n",
    "    'dell inspiron': 'dell',\n",
    "    'e6520': 'dell',\n",
    "    'precision 5770': 'dell',\n",
    "    'latitude': 'dell',\n",
    "    'asus vivobook l203': 'asus',\n",
    "    'hp elitebook': 'hp',\n",
    "    'ideapad 3': 'lenovo',\n",
    "    'lenovo thinkpad': 'lenovo',\n",
    "    'thinkpad l13 yoga': 'lenovo'\n",
    "    }\n",
    "    return correctBrands.get(model, pd.NA)\n",
    "\n",
    "mask = df['brand'].isnull()\n",
    "df.loc[mask, 'brand'] = df.loc[mask, 'model'].apply(imputeBrands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `brand` and `model` columns\n",
    "\n",
    "The `brand` and `model` column share a lot of related data, and in some cases the value is in the wrong column or there is data duplication.\n",
    "\n",
    "Lets look at the case where the model is in the brand column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['brand'].isin(['latitude', 'toughbook'])\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take care of this by replacing latitude and toughbook with their appropriate brands and then appending latitude or toughbook to the start of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: Some rows contain toughbook or latitude in their brand column even though they are model names.\n",
    "#            I move these to their appropriate model column and update the brand column with the correct brand.\n",
    "def standardiseToughbookLatitude(row):\n",
    "    patterns = {\n",
    "        r'(?P<model>toughbook)': 'panasonic',\n",
    "        r'(?P<model>latitude)': 'dell'\n",
    "    }\n",
    "    for pattern, mappedValue in patterns.items():\n",
    "        if match := re.search(pattern, str(row['brand'])):\n",
    "            row['brand'] = mappedValue\n",
    "\n",
    "            if match.groupdict().get('model') not in row['model']:\n",
    "                row['model'] = match.groupdict().get('model') + ' ' + row['model']\n",
    "    return row\n",
    "df = df.apply(standardiseToughbookLatitude, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the effects of executing this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, there are also many cases where the `brand` is in the `model` column. This is data duplication and serves no purpose to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: I remove all instances of the brand occurring in the model column.\n",
    "#            This is redundant data and serves no purpose\n",
    "def removeBrandFromModel(row):\n",
    "    if not pd.isna(row['model']) and not pd.isna(row['brand']):\n",
    "        if row['brand'] in str(row['model']):\n",
    "            row['model'] = row['model'].replace(row['brand'], '')\n",
    "    return row\n",
    "df = df.apply(removeBrandFromModel, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `color` column\n",
    "\n",
    "Before we begin with standardising this column, we need to take care of a few non-standard values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_colors = {\n",
    "    'rgb backlit': {\n",
    "        'special_features': 'rgb backlit keyboard'\n",
    "    },\n",
    "    'touchscreen': {\n",
    "        'special_features': 'touchscreen'\n",
    "    },\n",
    "    'evo i7-1260p': {\n",
    "        'cpu': 'evo i7-1260p'\n",
    "    }\n",
    "}\n",
    "manualClean(df, problematic_colors, 'color', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we begin with standardising the colours of the laptops. \n",
    "\n",
    "Lets have a look at how many colours there currently are in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['color'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats way too many colours, some of there are also very similar.\n",
    "\n",
    "To standardise them, we define a dictionary containing regex that match certain colours and then replace them with the correct colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning: Reduces variability in the dataset due to misspellings, variations and interpretations\n",
    "#            Reducuing the variability also helps with categorization which is use for data exploration and visualization\n",
    "#            A part of this process also ensures contextual standardization which is crucial where colours are named uniquely \n",
    "#            for marketing or specific contexts (e.g. thunder black or platinum titan)\n",
    "#            Need to first handle the cases that contain multiple conflicting standard colours\n",
    "colorPatterns = {\n",
    "    r\".*(information not available|acronym).*\": pd.NA,             #TODO Need to justify these\n",
    "    r'titanium blue-black-dark blue-black|black/white': 'black',    #\n",
    "    r'cover: red ; inner/keyboard: black': 'red',                   #\n",
    "    r'ice blue \\+ iron grey': 'grey',                               #TODO\n",
    "    r\".*(silver|sliver|aluminum|platinum|light titan|platinum titan).*\": \"silver\",\n",
    "    r\".*(black|dark side of the moon|thunder balck|carbon fiber).*\": \"black\",\n",
    "    r\".*(white).*\": \"white\",\n",
    "    r\".*(grey|gray|gary|graphite|mercury|dark ash|dark metallic moon).*\": \"grey\",\n",
    "    r\".*(red).*\": \"red\",\n",
    "    r\".*(blue|cobalt|sky|dark teal|apollo|midnight).*\": \"blue\",\n",
    "    r\".*(green|sage|soft mint|dark moss).*\": \"green\",\n",
    "    r\".*(punk pink|electro punk|rose gold).*\": \"pink\",\n",
    "    r\".*(gold).*\": \"gold\",\n",
    "    r\".*(almond|beige mousse|lunar light|dune).*\": \"beige\"          #TODO Need to justify this\n",
    "}\n",
    "df['color'] = df['color'].replace(colorPatterns, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many colours are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['color'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `screen_size` and `price` columns\n",
    "\n",
    "These two columns store very similar data and the data is entirely clean. There are no bad values and hence the extraction of the values is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNumericalValue(price):\n",
    "    if pd.isna(price):\n",
    "        return np.NaN\n",
    "    numerical_value = re.search(r'(?P<value>\\d+(?:\\.\\d+)?)', price.replace(\",\", \"\")).groupdict().get('value')\n",
    "    return float(numerical_value)\n",
    "\n",
    "# Remove 'inches' from screen sizes and '$' from prices. Convert these values to floats\n",
    "# Reasoning: Removing 'inches' and '$' allows the values to be converted from a categorical string type to a continuous scale.\n",
    "#            This allows for better data visualisation.\n",
    "df['screen_size'] = df['screen_size'].apply(extractNumericalValue)\n",
    "df['price'] = df['price'].apply(extractNumericalValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `ram` and `harddisk` column\n",
    "\n",
    "These two columns are both in the same unit i.e. Bytes.\n",
    "This makes it convenient to handle the standardisation of both of them in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RAM')\n",
    "print(df['ram'].unique())\n",
    "print()\n",
    "print('Harddisk')\n",
    "print(df['harddisk'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is however a slight difference - It is extremely unlikely for a laptop to have over a terabyte of ram but possible to have a terabyte of harddisk. This is evident in the dataset.\n",
    "\n",
    "A good way to handle this is by having a single function which handles converting and extracting the numerical value. Lets call it `convert_to_gb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_gb(size, tb_possible = False):\n",
    "    # TODO check the correctness of this function, especially with NaNs\n",
    "    if pd.isna(size): # Check for NaN\n",
    "        return np.NaN\n",
    "\n",
    "    if not isinstance(size, str):\n",
    "        return round(float(size))\n",
    "\n",
    "    if size.endswith('tb'):\n",
    "        return round(float(size.replace('tb', '')) * 1000)\n",
    "    \n",
    "    if size.endswith('gb'):\n",
    "        return round(float(size.replace('gb', '')))\n",
    "    \n",
    "    elif size.endswith('mb'):\n",
    "        return round(float(size.replace('mb', '')))\n",
    "    else:\n",
    "        if tb_possible and float(size) < 16:\n",
    "            return float(size) * 1000\n",
    "        else: \n",
    "            return float(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that can smoothly convert all the values into gigabytes we can just apply it to every value in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust harddisk sizes to be more consistent\n",
    "df['harddisk'] = df['harddisk'].apply(convert_to_gb, tb_possible=True)\n",
    "df['ram'] = df['ram'].apply(convert_to_gb, tb_possible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the values look like now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RAM')\n",
    "print(df['ram'].unique())\n",
    "print()\n",
    "print('Harddisk')\n",
    "print(df['harddisk'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `OS` column\n",
    "\n",
    "First lets have a look at what sort of data is in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to how we handled colours, we can create a regex dictionary to map all the current values to a more standardised system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert os systems to standard systems\n",
    "# Reasoning: Removes unnecessary details and simplifies the OSs to simplistic values\n",
    "osPatterns = {\n",
    "    r\".*(10 pro).*\": \"windows 10 pro\",\n",
    "    r\".*(11 pro).*\": \"windows 11 pro\",\n",
    "    r\".*(windows 10|win 10).*\": \"windows 10 home\",\n",
    "    r\".*(windows 11 home|win 11 multi-home).*\": \"windows 11 home\",\n",
    "    r'.*(windows 11 s).*': 'windows 11',\n",
    "    r\".*(windows 7 professional).*\": \"windows 7 pro\",\n",
    "    r'.*(windows 7 home).*': 'windows 7',\n",
    "    r'.*(windows pro).*': 'windows',\n",
    "    r'.*(macos 10.12 sierra).*': 'mac os sierra',\n",
    "    r'.*(macos 12 monterey).*': 'mac os monterey',\n",
    "    r'.*(unknown|no).*': pd.NA\n",
    "}\n",
    "df['OS'] = df['OS'].replace(osPatterns, regex=True)\n",
    "\n",
    "#TODO Dont just delete, keep it, just delete the column value\n",
    "df = df[df['OS'] != 'hp thinpro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what the values look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `cpu` column\n",
    "\n",
    "First let's have a look at what sort of data is in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cpu'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is immediately obvious is that some rows contain the cpu speed of the given processor. This can be extracted into the `cpu_speed` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cpu'] = df['cpu'].str.replace(r'_|-', ' ', regex=True)\n",
    "\n",
    "def extractCPUspeed(row):\n",
    "    pattern = r'(?: ?(?P<cpu_speed>\\d\\.\\d) ?ghz ?)'\n",
    "    if match := re.search(pattern, str(row['cpu'])):\n",
    "        if speed := match.groupdict().get('cpu_speed'):\n",
    "            row['cpu_speed'] = speed\n",
    "            row['cpu'] = re.sub(pattern, '', str(row['cpu']))\n",
    "    return row\n",
    "df = df.apply(extractCPUspeed, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cpu'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the structure and format of the values, we can construct a good way to split the values into 3 component columns:\n",
    "- `cpu_brand`\n",
    "- `cpu_series`\n",
    "- `cpu_model`\n",
    "\n",
    "We can use a similar regex dictionary structure as before but this time we can sub-divide the dictionary based on the brand. This will be useful for imputing the brand when it isnt specified in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandPatterns = {\n",
    "    'intel': {\n",
    "        r'^(?:intel )?core (?P<cpu_series>i[3579]( extreme)?)(?: family)?$',\n",
    "        r'^(?:intel )?core\\s?(?P<cpu_series>i[3579])(?:[ -])(?P<cpu_model>(?:\\d{3,5}(?:[uthxyqmk]{1,2}))|(?:\\d{4}g\\d))$',\n",
    "        r'^(?:intel )?(?:(?P<cpu_series>celeron|pentium)\\s?)(?P<cpu_model>[np]\\d{4}|\\d{4}u|n|d|4)?$',\n",
    "        r'(?:intel )?(?P<cpu_series>core m\\d?)(?: (?P<cpu_model>8100y|5y10))?',\n",
    "        r'^(?:intel )?(?:(?P<cpu_series>mobile) cpu)$',\n",
    "        r'^(?:intel )?(?P<cpu_series>atom|xeon)$',\n",
    "        r'^(?P<cpu_series>8032)$',\n",
    "        r'^(?:intel )?(?P<cpu_series>core (?:2 duo|2|duo))(?: quad)?(?: (?P<cpu_model>p\\d{4}))?$',\n",
    "        r'^(?P<cpu_series>evo i7) (?P<cpu_model>1260p)$',\n",
    "        r'^(?P<cpu_series>atom) (?P<cpu_model>z8350)$'\n",
    "    },\n",
    "    'arm': {\n",
    "        r'(?P<cpu_series>cortex)(?: (?P<cpu_model>a\\d{1,2}))?',\n",
    "        r'^arm (?P<cpu_series>7100)$'\n",
    "    },\n",
    "    'amd': {\n",
    "        r'^(?:amd )?(?P<cpu_series>ryzen\\s?[3579])(?:\\s(?P<cpu_model>\\d{4}[hux]))?$',\n",
    "        r'(?:amd )?(?:kabini )?(?P<cpu_series>[ar] series|a\\d{1,2})(?: (?P<cpu_model>\\d{4}[km]))?',\n",
    "        r'^(?P<cpu_series>athlon(?: silver)?)(?:\\s(?P<cpu_model>\\d{4}u))?$',\n",
    "        r'^a series dual core (?P<cpu_series>a4) (?P<cpu_model>3300m)$'\n",
    "    },\n",
    "    'mediatek': {\n",
    "        r'^(?:mediatek)[ _](?P<cpu_model>.*)$',\n",
    "    },\n",
    "    'apple': {\n",
    "        r'^(?:apple )(?P<cpu_series>m[12])?$',\n",
    "    },\n",
    "\n",
    "    'snapdragon': {\n",
    "        r'^snapdragon$',\n",
    "    },\n",
    "    'motorola': {\n",
    "        r'^(?P<cpu_series>68000)$',\n",
    "    },\n",
    "    pd.NA: {\n",
    "        r'^unknown|others$',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dictionary which encodes all the patterns. We can use a function to apply all the regexes to each row of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCPUdetails(row):\n",
    "    for brand, patterns in brandPatterns.items():\n",
    "            for pattern in patterns:\n",
    "                if match := re.search(pattern, str(row['cpu'])):\n",
    "                    row['cpu_brand'] = brand\n",
    "\n",
    "                    if match.groupdict().get('cpu_series') and pd.isna(row['cpu_series']):\n",
    "                        row['cpu_series'] = match.group('cpu_series')\n",
    "\n",
    "                    if match.groupdict().get('cpu_model') and pd.isna(row['cpu_model']):\n",
    "                        row['cpu_model'] = match.group('cpu_model')         \n",
    "    return row\n",
    "\n",
    "df = df.apply(extractCPUdetails, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `special_features` column\n",
    "\n",
    "Similarly to before, we construct a regex dictionary which stores all the mappings of the special features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    r'(anti-? ?(?:gla(?:re)?)|reflection)': 'anti-glare',\n",
    "    r'fingerprint': 'fingerprint sensor',\n",
    "    r'bezel|(?:infinity|nano)edge|narrow': 'thin bezel',\n",
    "    r'backli(?:gh)?t': 'backlit keyboard',\n",
    "    r'^(?:stylus )?pen$|active stylus': 'stylus',\n",
    "    r'(?:high definition|hd) audio': 'hd audio',\n",
    "    r'support stylus': 'stylus support',\n",
    "    r'dolby': 'dolby audio',\n",
    "    r'spill[ -]resistant': 'spill-resistant',\n",
    "    r'stereo|speakers': 'stereo speakers',\n",
    "    r'multi[ -]touch': 'multi-touch',\n",
    "    r'chiclet': 'chiclet keyboard',\n",
    "    r'alexa': 'amazon alexa',\n",
    "    r'corning gorilla glass': 'corning gorilla glass',\n",
    "    r'water proof|water resistant': 'water-resistant',\n",
    "    r'lightweight|light and compact design': 'lightweight',\n",
    "    r'killer wifi 6e': 'wifi',\n",
    "    r'touch ?screen': 'touchscreen',\n",
    "    r'anti-ghost key': 'anti-ghost keys',\n",
    "    r'keypad': 'numeric keypad',\n",
    "    r'bluetooth|memory card slot|2-in-1|trackpoint|anti-smudge|miracast|microphone|security slot|camera|detachable|144hz refresh rate|ergonomic': '_KEEP_'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create a function which will iterate through all the rows, converting all the special features into the standardised values.\n",
    "\n",
    "Additionally, lets replace all occurrences of 'wifi & bluetooth' with 'wifi,bluetooth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['special_features'] = df['special_features'].replace(r'wifi & bluetooth', 'wifi,bluetooth', regex=True)\n",
    "\n",
    "def enumSpecialFeatures(row):\n",
    "    if pd.isna(row['special_features']):\n",
    "        return row\n",
    "\n",
    "    #Split original rows based on commas and remove all leading and trailing whitespace\n",
    "    features_split = str(row['special_features']).split(',')\n",
    "    stripped_features = [feature.strip() for feature in features_split]\n",
    "\n",
    "    # Apply each regex to each feature listed in the row\n",
    "    for idx, feature in enumerate(stripped_features):\n",
    "        for pattern, mapped_value in patterns.items():\n",
    "            if re.search(pattern, feature):\n",
    "                stripped_features[idx] = mapped_value if mapped_value != '_KEEP_' else feature\n",
    "                break\n",
    "        # Else is executed if the break statement was never reached.\n",
    "        else:\n",
    "            stripped_features[idx] = pd.NA\n",
    "\n",
    "    # Remove all empty string and NaN from the special features\n",
    "    condensed_features = [feature for feature in stripped_features if (not pd.isna(feature) and feature != '')]\n",
    "\n",
    "    # Turn into set to remove duplicates, convert back to list, sort the list\n",
    "    # Reasoning: Sorting the list will be helpful later when checking for duplicate \n",
    "    #            laptops that just had their special features in different orders.\n",
    "    condensed_features = sorted(list(set(condensed_features)))\n",
    "\n",
    "    # Convert the list of features back into comma seperated strings\n",
    "    # Lists containing no features are mapped to NaN\n",
    "    row['special_features'] = \",\".join(condensed_features) if len(condensed_features) > 0 else pd.NA\n",
    "\n",
    "    return row\n",
    "\n",
    "df = df.apply(enumSpecialFeatures, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `cpu_speed` column\n",
    "\n",
    "Before beginning standardisation, lets examine the current data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cpu_speed'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the values are all in a fairly standard format. Since there are currently no processors operating faster than 10Ghz, it is fair to say any speed above 10 is written in MHz and should be converted to GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust CPU speeds to be more consistent with the same units\n",
    "def standardiseCPU(cpu_speed):\n",
    "    match = re.search(r'\\d+(\\.\\d+)?', str(cpu_speed))\n",
    "    if match:\n",
    "        value = float(match.group())\n",
    "        return value / 1000 if value > 10 else value\n",
    "\n",
    "df['cpu_speed'] = df['cpu_speed'].apply(standardiseCPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `graphics` column\n",
    "\n",
    "Examining the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['graphics'].unique())\n",
    "\n",
    "print(df['graphics_coprocessor'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident this data is very unstructured.\n",
    "- The `graphics` should only contain integrated or dedicated however it contains other values\n",
    "- The values in the `graphics_coprocessor` have a similar format but the data would be better off extracted into finer columns.\n",
    "\n",
    "Ideally, for each laptop we should store\n",
    "- `graphics` - 'integrated' or 'dedicated' graphics\n",
    "- `graphics_brand` - The brand of the GPU\n",
    "- `graphics_details` - The specific details of the gpu\n",
    "\n",
    "This would allow the original data to remain intact whilst also allowing laptops to be compared based on specific sub-parameters.\n",
    "\n",
    "Before we begin standardising, we need to address some specific values. These are mostly values that do not belong in the `graphics` columns and instead belong in the `cpu` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_gpus = {\n",
    "        'amd athlon silver': {\n",
    "            'cpu_brand': 'amd',\n",
    "            'cpu_series': 'athlon silver',\n",
    "        },\n",
    "        'inter core i7-8650u': {\n",
    "            'cpu_brand': 'intel',\n",
    "            'cpu_series': 'i7',\n",
    "            'cpu_model': '8650u',\n",
    "        },\n",
    "        'amd athlon': {\n",
    "            'cpu_brand': 'amd',\n",
    "            'cpu_series': 'athlon',\n",
    "        },\n",
    "        'intel celeron': {  \n",
    "            'cpu_brand': 'intel',\n",
    "            'cpu_series': 'celeron',\n",
    "        },\n",
    "        'm1 pro': {\n",
    "            'cpu_brand': 'apple',\n",
    "            'cpu_series': 'm1 pro',\n",
    "            'cpu_model': pd.NA,\n",
    "        },\n",
    "        'xps9300-7909slv-pus': {\n",
    "            'model': 'xps9300-7909slv-pus',\n",
    "        },\n",
    "        'integrated intel iris xe plus gpu': {\n",
    "            'graphics': 'dedicated',\n",
    "        }\n",
    "    }\n",
    "manualClean(df, problematic_gpus, 'graphics_coprocessor', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous methods, the way to approach this column is to use regex.\n",
    "This time however, for each value, it would be ideal to impute missing `graphics` and `graphics_brand` values. \n",
    "\n",
    "This is done by storing each regex in a sort of directory. This means that whenever a regex matches, it is possible to infer the brand and graphics type by traversing up the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphicsPatterns = {\n",
    "    'dedicated': {\n",
    "        'nvidia': {\n",
    "            r'nvidia optimus graphics': 'optimus graphics',\n",
    "            r'(?P<graphics_details>quadro rtx\\s*\\d{4}(?:\\s*(?:ti|super))?)': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>rtx\\s*\\d{4}(?:\\s*(?:ti|super))?)': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>rtx\\s*a\\d{1,2}00)': '__EXTRACT__',\n",
    "            r'nvidia geforce (?P<graphics_details>mx\\d{2}0)': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>(?:quadro )?t\\d{3,4})': '__EXTRACT__',\n",
    "            r'geforce(?: rtx| gtx)?(?: (?P<graphics_details>\\d{4}(?:\\s?(?:oc|ti))?|a3000|940mx))': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>quadro (?:p\\d{3,4}|k?\\d{4}m?))': '__EXTRACT__',\n",
    "            r'^(?P<graphics_details>qn20-m1-r)$': '__EXTRACT__'\n",
    "        },  \n",
    "        'amd': {\n",
    "            r'amd (?P<graphics_details>radeon r\\d)': '__EXTRACT__',\n",
    "            r'wx vega': 'radeon pro wx vega m gl',\n",
    "            r'(?:amd )?(?P<graphics_details>(?:ati mobility )?radeon(?: graphics)?(?: (?:hd|pro|rx))? \\d{3,4}(?!.*m))': '__EXTRACT__'\n",
    "        },\n",
    "        'intel': {\n",
    "            r'intel (?P<graphics_details>arc a\\d{3}m)': '__EXTRACT__'\n",
    "        }\n",
    "    },\n",
    "    'integrated': {\n",
    "        'intel': {\n",
    "            r'(?P<graphics_details>uhd(?: graphics)?(?: premium| \\d{3})?)': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>hd graphics(?: \\d{3,4})?)(?!nvidia)': '__EXTRACT__',\n",
    "            r'iris plus': 'iris xe plus graphics',\n",
    "            r'iris(?!.*gpu)|intel xe': 'iris xe graphics',\n",
    "            r'intel (?P<graphics_details>hd(?: \\d{3,4})?)': '__EXTRACT__',\n",
    "            r'(?P<graphics_details>gt2 graphics)': '__EXTRACT__',\n",
    "            r'^(?P<graphics_details>hd integrated graphics)$': '__EXTRACT__',\n",
    "            r'intel (?P<graphics_details>\\d{3}u)': '__EXTRACT__'\n",
    "        },\n",
    "        'nvidia': {\n",
    "            r'nvidia geforce gtx (?P<graphics_details>\\d{3}m)': '__EXTRACT__'\n",
    "        },\n",
    "        'amd': {\n",
    "            r'amd (?P<graphics_details>radeon (?:rx )?vega (?:3|8|9|11))': '__EXTRACT__',\n",
    "            r'^(?:integrated )?(?:amd )?radeon(?: graphics)?$': 'radeon graphics',\n",
    "            r'(?P<graphics_details>radeon (?:(?:rx|hd) )?\\d{3,4}m)': '__EXTRACT__',\n",
    "            r'^amd (?P<graphics_details>integrated graphics)$': '__EXTRACT__',\n",
    "            r'^amd (?P<graphics_details>radeon graphics 5)$': '__EXTRACT__'\n",
    "        },\n",
    "        'mediatek': {\n",
    "            r'mediatek': pd.NA\n",
    "        },\n",
    "        'imagination technologies': {\n",
    "            r'^(?P<graphics_details>powervr gx6250)$': '__EXTRACT__'\n",
    "        },\n",
    "        'apple': {\n",
    "            r'apple integrated graphics': pd.NA\n",
    "        },\n",
    "        'qualcomm': {\n",
    "            r'(?P<graphics_details>adreno 618)': '__EXTRACT__'\n",
    "        },\n",
    "        'arm': {\n",
    "            r'(?P<graphics_details>mali-g[57]2 (?:mp3|2ee mc2)?)': '__EXTRACT__'\n",
    "        },\n",
    "        pd.NA: {\n",
    "            r'^embedded$|^integrated(?: ?graphics)?$|^shared$': pd.NA\n",
    "        }\n",
    "    },\n",
    "    '__KEEP__': {\n",
    "        'nvidia': {\n",
    "            r'nvidia': pd.NA\n",
    "        },\n",
    "        'intel': {\n",
    "            r'intel': pd.NA\n",
    "        }\n",
    "    },\n",
    "    pd.NA: {\n",
    "        pd.NA: {\n",
    "            r'^$': pd.NA,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets define a function that will use this dictionary to extract the values into the finer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractGPUdetails(row):\n",
    "    for graphics_type, graphics_brands in graphicsPatterns.items():\n",
    "        for graphics_brand, patterns in graphics_brands.items():\n",
    "            for pattern, overwriteValue in patterns.items():\n",
    "                if match := re.search(pattern, str(row['graphics_coprocessor'])):\n",
    "                    if graphics_type != '__KEEP__':\n",
    "                        row['graphics'] = graphics_type\n",
    "                    row['graphics_brand'] = graphics_brand\n",
    "\n",
    "                    if (pd.isna(row['graphics_details']) or row['graphics_details'] == ''):\n",
    "                        if details := match.groupdict().get('graphics_details'):\n",
    "                            row['graphics_details'] = details\n",
    "                        else:\n",
    "                            row['graphics_details'] = overwriteValue\n",
    "                    \n",
    "    row['graphics_coprocessor'] = pd.NA\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this function we can define the function that will wrap all this function together.\n",
    "\n",
    "It will\n",
    "- Fix some spelling errors\n",
    "- Trim each value to remove extra whitespace\n",
    "- Move all values that are not integrated or dedicated into the `graphics_coprocessor` column\n",
    "- Remove all non-alphanumeric characters\n",
    "- Execute `extractGPUDetails()` on the `graphics_coprocessor` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spellings and duplication\n",
    "columns = ['graphics', 'graphics_coprocessor']\n",
    "replacements = {r'integreted|intergrated': 'integrated', \n",
    "                r'integrated, dedicated': 'dedicated',\n",
    "                r'geforcer': 'geforce',\n",
    "                r'grpahics|\\bgraphic\\b': 'graphics',\n",
    "                r'\\bx\\b': 'xe',\n",
    "                r'rtx(?! )': 'rtx ',\n",
    "                r'\\bgt\\b': 'gtx'}\n",
    "df[columns] = df[columns].replace(replacements, regex=True)\n",
    "\n",
    "# Reasoning: Remove unnecessary whitespace from the value\n",
    "df[columns] = df[columns].replace('\\s+', ' ', regex=True)\n",
    "for column in columns:\n",
    "    df[column] = df[column].str.strip(' ')\n",
    "\n",
    "# Move gpu info to the other column\n",
    "def moveGPUinfo(row):\n",
    "    if pd.notna(row['graphics']):\n",
    "        if row['graphics'] not in ['integrated', 'dedicated']:\n",
    "            row['graphics_coprocessor'] = row['graphics']\n",
    "            row['graphics'] = pd.NA\n",
    "    return row\n",
    "df = df.apply(moveGPUinfo, axis=1)\n",
    "\n",
    "# Remove all non-alphanumeric characters\n",
    "df['graphics_coprocessor'] = df['graphics_coprocessor'].replace(r'[^A-Za-z0-9, \\-]', '', regex=True)\n",
    "\n",
    "df = df.apply(extractGPUdetails, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNameDict = {\n",
    "    'screen_size': 'screen_size_inches',\n",
    "    'color': 'colour',\n",
    "    'harddisk': 'harddisk_gb',\n",
    "    'ram': 'ram_gb',\n",
    "    'cpu_speed': 'cpu_speed_ghz',\n",
    "    'price': 'price_usd',\n",
    "    'OS': 'os'\n",
    "}\n",
    "df = df.rename(columns=columnNameDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'brand', \n",
    "    'model', \n",
    "    'colour', \n",
    "    'cpu_brand', \n",
    "    'cpu_series', \n",
    "    'cpu_model', \n",
    "    'os', \n",
    "    'special_features', \n",
    "    'graphics', \n",
    "    'graphics_brand', \n",
    "    'graphics_details'\n",
    "]\n",
    "integer_columns = [\n",
    "    'harddisk_gb', \n",
    "    'ram_gb'\n",
    "]\n",
    "float_columns = [\n",
    "    'screen_size_inches', \n",
    "    'cpu_speed_ghz', \n",
    "    'rating',\n",
    "    'price_usd'        \n",
    "]\n",
    "\n",
    "# Drop old columns\n",
    "df = df.drop(columns=['cpu', 'graphics_coprocessor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we export the data lets view a summary of the whole data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", df.shape)\n",
    "print(\"\".join(['-'] * 40))\n",
    "print(df.dtypes)\n",
    "print(\"\".join(['-'] * 40))\n",
    "printNumEmpty(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear there are a few columns (`cpu_model`, `cpu_speed_ghz`, `rating`, `graphics_details`, `graphics_brand`, `special_features` and `color`) which are more than `10%` empty. \n",
    "\n",
    "The most concerning column is `cpu_speed_ghz`. This is a numerical column and would have been useful for comparing the relative performance of CPUs of laptops. This column will more than likely be dropped upon further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the types for all the columns and fill missing values with flag value\n",
    "df[categorical_columns] = df[categorical_columns].fillna('_MISSING_')\n",
    "df[categorical_columns] = df[categorical_columns].astype(str)\n",
    "df[integer_columns] = df[integer_columns].astype('Int64')\n",
    "df[float_columns] = df[float_columns].astype(float)\n",
    "\n",
    "# Clean string data\n",
    "df[categorical_columns] = df[categorical_columns].replace('\\s+', ' ', regex=True)\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: x.str.strip(', '))\n",
    "\n",
    "df = df[df['model'] != '']\n",
    "\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", df.shape)\n",
    "print(\"\".join(['-'] * 40))\n",
    "print(df.dtypes)\n",
    "print(\"\".join(['-'] * 40))\n",
    "printNumEmpty(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "One last issue with the data in its current form is some value occur very few times in each column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['graphics_brand'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the visualisation quite unappealing. \n",
    "A way to solve this is by putting values that dont meet a frequency threshold into a seperate category `'OTHER'`\n",
    "\n",
    "There is no accepted threshold, hence these thresholds have been determined experimentally to roughly leave a maximum of 10 values. \n",
    "\n",
    "The thresholds are stored in a dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'brand': 0.005,\n",
    "    'colour': 0.005,\n",
    "    'cpu_brand': 0.01,\n",
    "    'os': 0.01,\n",
    "    'graphics_brand': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column has its values adjusted with the following subroutine using the previously defined thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df[:]\n",
    "\n",
    "other_value = 'OTHER'\n",
    "\n",
    "for column, cutoff in columns.items():\n",
    "    \n",
    "    value_counts = df_grouped[column].value_counts()\n",
    "    non_empty_rows = df_grouped.shape[0] - value_counts.get('_MISSING_', 0)\n",
    "\n",
    "    def replace_with_other(value):\n",
    "        if value == '_MISSING_':\n",
    "            return value\n",
    "        return other_value if value_counts.get(value, 0) / non_empty_rows < cutoff else value\n",
    "\n",
    "    df_grouped[column] = df_grouped[column].apply(replace_with_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by binning (discrete values)\n",
    "\n",
    "For some columns such as `cpu_series` it would be better to group the values by binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['cpu_series'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This because the two main cpu brands are Intel and AMD, hence only their main lineup of CPUs could be left in the dataset and the rest could be placed into OTHER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'cpu_series'\n",
    "\n",
    "KEEP = ['i3', 'i5', 'i7', 'i9', 'celeron', 'pentium', 'ryzen 3', 'ryzen 5', 'ryzen 7', 'ryzen 9']\n",
    "cpu_dict = {\n",
    "    'INTEL_OTHER': ['mobile', 'core m', 'atom', 'xeon', 'core 2 duo', 'i7 extreme', 'evo i7', 'core m3', 'core duo'],\n",
    "\n",
    "    'AMD_OTHER': ['a series', 'r series', 'athlon', 'athlon silver', 'a4', 'a10', 'a6'],\n",
    "}\n",
    "\n",
    "def replace_with_other(value):\n",
    "    if value == '_MISSING_' or value in KEEP:\n",
    "        return value\n",
    "    \n",
    "    for key, list in cpu_dict.items():\n",
    "        if value in list:\n",
    "            return key\n",
    "    return 'OTHER'\n",
    "\n",
    "df_grouped[column] = df_grouped[column].apply(replace_with_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['cpu_series'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by binning (ranges)\n",
    "\n",
    "Looking at the numerical columns, there are only 2 columns that could be binned, these are `ram_gb` and `harddisk_gb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['harddisk_gb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(DEFAULT_OUT_FILEPATH) as writer:  \n",
    "    df.to_excel(writer, sheet_name='amazon laptop 2023', index=False)\n",
    "    df_grouped.to_excel(writer, sheet_name='amazon laptop 2023 grouped', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_excel('amazon_laptop_2023_cleaned.xlsx')\n",
    "\n",
    "numerical_columns = integer_columns + float_columns\n",
    "\n",
    "# Labels for the subplots\n",
    "labels = [\n",
    "    ('Boxplot of Hard Disk Capacity', 'Amount (GB)', 30, 0.04),\n",
    "    ('Boxplot of RAM Capacity', 'Amount (GB)', 30, 0.02),\n",
    "    ('Boxplot of Screen Size', 'Size (inches)', 32, 0),\n",
    "    ('Boxplot of CPU Speed', 'Speed (GHz)', 30, 0.045),\n",
    "    ('Boxplot of Rating', 'Rating (1 - 5)', 32, 0.04),\n",
    "    ('Boxplot of Price', 'Price (US Dollars)', 38, 0.09)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=100)\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (column, (title, ylabel, labelpad, vertical_shift)) in enumerate(zip(numerical_columns, labels)):\n",
    "    sns.boxplot(y=df[column], ax=axes[i])\n",
    "    axes[i].set_title(title)\n",
    "    y_label = axes[i].set_ylabel(ylabel, rotation=0, labelpad=labelpad)\n",
    "    y_label.set_y(y_label.get_position()[1] + vertical_shift)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visualisations/boxplot_numerical.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
